{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i1KCiiu4laq",
        "outputId": "efc8844d-a5de-42dc-e8d8-6091f9bf67c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "root\n",
            " |-- visitor_id: integer (nullable = true)\n",
            " |-- item_id: integer (nullable = true)\n",
            " |-- event_ts: timestamp (nullable = true)\n",
            " |-- category_id: integer (nullable = true)\n",
            " |-- is_available: integer (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- hour: integer (nullable = true)\n",
            " |-- weekday: string (nullable = true)\n",
            " |-- item_view: long (nullable = true)\n",
            " |-- item_addtocart: long (nullable = true)\n",
            " |-- item_transaction: long (nullable = true)\n",
            " |-- item_ctr: double (nullable = true)\n",
            " |-- user_view: long (nullable = true)\n",
            " |-- user_addtocart: long (nullable = true)\n",
            " |-- user_transaction: long (nullable = true)\n",
            " |-- user_conv_rate: double (nullable = true)\n",
            "\n",
            "Line : 2751205\n",
            "+----------+-------+-------------------+-----------+------------+----------+----+-------+---------+--------------+----------------+--------------------+---------+--------------+----------------+--------------+\n",
            "|visitor_id|item_id|           event_ts|category_id|is_available|event_type|hour|weekday|item_view|item_addtocart|item_transaction|            item_ctr|user_view|user_addtocart|user_transaction|user_conv_rate|\n",
            "+----------+-------+-------------------+-----------+------------+----------+----+-------+---------+--------------+----------------+--------------------+---------+--------------+----------------+--------------+\n",
            "|         0|  67045|2015-09-11 20:55:17|        333|           0|      view|  20|    Fri|      202|             2|               1|0.004950495049504...|        3|             0|               0|           0.0|\n",
            "|         0| 285930|2015-09-11 20:49:49|         -1|           1|      view|  20|    Fri|       90|             4|               0|                 0.0|        3|             0|               0|           0.0|\n",
            "|         0| 357564|2015-09-11 20:52:39|        256|           0|      view|  20|    Fri|      267|             2|               2| 0.00749063670411985|        3|             0|               0|           0.0|\n",
            "|         2| 342816|2015-08-07 18:17:24|        444|           1|      view|  18|    Fri|       96|             8|               4|0.041666666666666664|        8|             0|               0|           0.0|\n",
            "|         2| 216305|2015-08-07 18:01:08|        299|           0|      view|  18|    Fri|      584|             2|               1|0.001712328767123...|        8|             0|               0|           0.0|\n",
            "+----------+-------+-------------------+-----------+------------+----------+----+-------+---------+--------------+----------------+--------------------+---------+--------------+----------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReloadTrainDF\").getOrCreate()\n",
        "\n",
        "train_df = spark.read.parquet(\"/content/drive/MyDrive/datasets/train_df_parquet\")\n",
        "\n",
        "train_df.printSchema()\n",
        "print(\"Line :\",train_df.count())\n",
        "train_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Recommender \"popularity-based\"\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "N=10\n",
        "topN = (\n",
        "    train_df.groupBy(\"item_id\")\n",
        "            .agg(F.sum(\"item_view\").alias(\"views\"))\n",
        "            .orderBy(\"views\", ascending=False)\n",
        "            .limit(N)\n",
        "    .select(\"item_id\")\n",
        "    .rdd.flatMap(lambda r: [r.item_id])\n",
        "    .collect()\n",
        ")\n",
        "\n",
        "# Popularity recommender function\n",
        "def recommend_popularity(user_id, k=N):\n",
        "  return topN[:k]\n",
        "\n",
        "# Example for an user\n",
        "print(\"Recommendations for user 10 :\", recommend_popularity(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZapB2H5m5qxx",
        "outputId": "5f0ee9d3-a082-4da9-f7ed-3fd6def8c0fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendations for user 10 : [187946, 461686, 5411, 370653, 219512, 298009, 96924, 309778, 257040, 384302]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Test split\n",
        "seed = 42\n",
        "\n",
        "train_df, test_df = train_df.randomSplit([0.8, 0.2], seed=seed)\n",
        "\n",
        "print(\"Total events :\", train_df.count() + test_df.count())\n",
        "print(\"Train events :\", train_df.count())\n",
        "print(\"Test  events :\", test_df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SVYHW039v_i",
        "outputId": "5001878e-02cb-4884-f0db-fafb39d3503e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total events : 2751205\n",
            "Train events : 2200937\n",
            "Test  events : 550268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# ground-truth\n",
        "gt = (\n",
        "    test_df.filter(\"event_type='transaction'\")\n",
        "           .groupBy(\"visitor_id\")\n",
        "           .agg(F.collect_set(\"item_id\").alias(\"true_items\"))\n",
        ")\n",
        "# Return the topN list\n",
        "pred = gt.selectExpr(\"visitor_id\", \"array(%s) as pred_items\" % \",\".join(map(str, topN)))\n",
        "\n",
        "recall_udf = F.udf(lambda t,p: float(len(set(t) & set(p))) /\n",
        "                   len(t) if t else 0.0, DoubleType())\n",
        "\n",
        "eval_df = (\n",
        "    gt.join(pred, \"visitor_id\")\n",
        "      .withColumn(\"recall_at_10\", recall_udf(\"true_items\",\"pred_items\"))\n",
        ")\n",
        "\n",
        "eval_df = (\n",
        "    gt.join(pred, \"visitor_id\")\n",
        "      .withColumn(\"recall_at_10\", recall_udf(\"true_items\",\"pred_items\"))\n",
        ")\n",
        "\n",
        "avg_recall = eval_df.agg(F.avg(\"recall_at_10\")).first()[0]\n",
        "print(f\"Recall@10 mean (80/20 split) : {avg_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgpEkb82-kaG",
        "outputId": "1ce9b5ba-6bba-4330-f879-8a2a7b115dbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@10 mean (80/20 split) : 0.0139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Co-occurence recommender\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "views = (\n",
        "    train_df\n",
        "      .select(\"visitor_id\",\"item_id\")\n",
        "      .distinct()\n",
        ")\n",
        "\n",
        "pairs = (\n",
        "    views.alias(\"a\")\n",
        "         .join(views.alias(\"b\"), on=\"visitor_id\")\n",
        "         .filter(\"a.item_id < b.item_id\")\n",
        "         .groupBy(\"a.item_id\",\"b.item_id\")\n",
        "         .count()\n",
        "         .withColumnRenamed(\"count\",\"co_count\")\n",
        ")\n",
        "\n",
        "sym_pairs = (\n",
        "    pairs\n",
        "      .select(F.col(\"a.item_id\").alias(\"src\"),\n",
        "              F.col(\"b.item_id\").alias(\"dst\"),\n",
        "              \"co_count\")\n",
        "    .union(\n",
        "      pairs\n",
        "       .select(F.col(\"b.item_id\").alias(\"src\"),\n",
        "               F.col(\"a.item_id\").alias(\"dst\"),\n",
        "               \"co_count\")\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "w = Window.partitionBy(\"src\").orderBy(F.desc(\"co_count\"))\n",
        "\n",
        "item_sims = (\n",
        "    sym_pairs\n",
        "      .withColumn(\"rank\", F.row_number().over(w))\n",
        "      .filter(\"rank <= 100\")\n",
        ")\n",
        "\n",
        "\n",
        "user_seen = views.alias(\"v\")\n",
        "\n",
        "\n",
        "recos = (\n",
        "    user_seen\n",
        "      .join(item_sims.alias(\"s\"),\n",
        "            user_seen.item_id == item_sims.src,\n",
        "            how=\"left\")\n",
        "      .select(\"visitor_id\",\"dst\",\"co_count\")\n",
        "      .groupBy(\"visitor_id\",\"dst\")\n",
        "      .agg(F.sum(\"co_count\").alias(\"score\"))\n",
        ")\n",
        "\n",
        "\n",
        "w2 = Window.partitionBy(\"visitor_id\").orderBy(F.desc(\"score\"))\n",
        "user_recos = (\n",
        "    recos\n",
        "      .withColumn(\"rank\", F.row_number().over(w2))\n",
        "      .filter(\"rank <= 10\")\n",
        "      .select(\"visitor_id\",\"dst\",\"score\")\n",
        "      .orderBy(\"visitor_id\",\"rank\")\n",
        ")\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import collect_list\n",
        "\n",
        "pred_cos = (\n",
        "    user_recos\n",
        "      .groupBy(\"visitor_id\")\n",
        "      .agg(collect_list(\"dst\").alias(\"pred_items\"))\n",
        ")"
      ],
      "metadata": {
        "id": "6ykqUixuADC7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import collect_set, when, col, array, lit, array_intersect, expr\n",
        "\n",
        "\n",
        "gt = (\n",
        "    test_df\n",
        "      .filter(\"event_type = 'transaction'\")\n",
        "      .groupBy(\"visitor_id\")\n",
        "      .agg(collect_set(\"item_id\").alias(\"true_items\"))\n",
        ")\n",
        "\n",
        "gt = gt.withColumn(\"true_items\", col(\"true_items\").cast(\"array<string>\"))\n",
        "\n",
        "k = 10\n",
        "pred_cos = pred_cos.withColumn(\n",
        "    \"pred_items\", when(col(\"pred_items\").isNull(), array().cast(\"array<string>\")).otherwise(col(\"pred_items\"))\n",
        ")\n",
        "\n",
        "eval_df = (\n",
        "    gt.join(pred_cos, on=\"visitor_id\", how=\"left\")\n",
        "      .withColumn(\"pred_k\", expr(f\"slice(pred_items, 1, {k})\"))\n",
        "      .withColumn(\"recall_at_10\",\n",
        "                  when(F.size(\"true_items\") > 0,\n",
        "                       F.size(array_intersect(\"true_items\", \"pred_k\")) / F.size(\"true_items\"))\n",
        "                  .otherwise(lit(0.0))\n",
        "      )\n",
        ")\n",
        "\n",
        "avg_recall = eval_df.agg(F.avg(\"recall_at_10\")).first()[0]\n",
        "print(f\"Recall@{k} co-occurrence : {avg_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "_-EZ7_EGHz-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ba3796-a880-4111-9672-82c7043ab527"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@10 co-occurrence : 0.2006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.functions import col, array, array_intersect, size, when, lit, expr, collect_set\n",
        "\n",
        "\n",
        "rating_df = (\n",
        "    train_df\n",
        "      .withColumn(\"user\",   col(\"visitor_id\").cast(\"long\"))\n",
        "      .withColumn(\"item\",   col(\"item_id\").cast(\"int\"))\n",
        "      .withColumn(\"rating\",\n",
        "           F.when(col(\"event_type\")==\"view\",        1.0)\n",
        "            .when(col(\"event_type\")==\"addtocart\",   3.0)\n",
        "            .when(col(\"event_type\")==\"transaction\", 5.0)\n",
        "            .otherwise(0.0)\n",
        "      )\n",
        "      .select(\"user\",\"item\",\"rating\")\n",
        ")\n",
        "\n",
        "sample_frac = 1.0\n",
        "sampled_df = rating_df.sample(fraction=sample_frac, seed=42).cache()\n",
        "sampled_df.count()\n",
        "\n",
        "als = ALS(\n",
        "    userCol=\"user\", itemCol=\"item\", ratingCol=\"rating\",\n",
        "    implicitPrefs=True,\n",
        "    rank=20,\n",
        "    maxIter=6,\n",
        "    regParam=0.1,\n",
        "    alpha=5.0,\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "model = als.fit(sampled_df)\n",
        "\n",
        "gt = (\n",
        "    test_df\n",
        "      .filter(col(\"event_type\")==\"transaction\")\n",
        "      .groupBy(\"visitor_id\")\n",
        "      .agg(collect_set(\"item_id\").alias(\"true_items\"))\n",
        "      .withColumn(\"true_items\", col(\"true_items\").cast(\"array<int>\"))\n",
        ")\n",
        "gt = gt.cache()\n",
        "gt.count()\n",
        "\n",
        "k = 10\n",
        "users = gt.select(col(\"visitor_id\").alias(\"user\"))\n",
        "recs = model.recommendForUserSubset(users, k)\n",
        "pred = (\n",
        "    recs\n",
        "      .select(\n",
        "         col(\"user\").alias(\"visitor_id\"),\n",
        "         expr(\"transform(recommendations, x -> x.item)\").alias(\"pred_items\")\n",
        "      )\n",
        "      .withColumn(\"pred_items\", F.coalesce(col(\"pred_items\"), array().cast(\"array<int>\")))\n",
        "      .withColumn(\"pred_k\", expr(f\"slice(pred_items,1,{k})\"))\n",
        "      .withColumn(\"pred_k\", when(col(\"pred_k\").isNull(), array().cast(\"array<int>\")).otherwise(col(\"pred_k\")))\n",
        "      .cache()\n",
        ")\n",
        "\n",
        "eval_df = (\n",
        "    gt\n",
        "      .join(F.broadcast(pred), on=\"visitor_id\", how=\"left\")\n",
        "      .withColumn(\"recall_at_10\",\n",
        "          when(\n",
        "            (size(\"true_items\") > 0) & (size(\"pred_k\") > 0),\n",
        "            size(array_intersect(\"true_items\", \"pred_k\")).cast(\"double\") /\n",
        "            size(\"true_items\").cast(\"double\")\n",
        "          ).otherwise(lit(0.0))\n",
        "      )\n",
        ")\n",
        "\n",
        "eval_df = eval_df.withColumn(\n",
        "    \"recall_at_10\",\n",
        "    when(col(\"recall_at_10\").isNull(), lit(0.0))\n",
        "    .when(col(\"recall_at_10\") < 0, lit(0.0))\n",
        "    .when(col(\"recall_at_10\") > 1, lit(1.0))\n",
        "    .otherwise(col(\"recall_at_10\"))\n",
        ")\n",
        "\n",
        "avg_recall = eval_df.agg(F.avg(\"recall_at_10\").alias(\"mean_recall\")) \\\n",
        "                   .first()[\"mean_recall\"]\n",
        "\n",
        "print(f\"Recall@{k} ALS = {avg_recall:.4f}\")\n",
        "\n",
        "gt.unpersist()\n",
        "pred.unpersist()"
      ],
      "metadata": {
        "id": "kMt0_LCuR6nD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c364c9-cbf3-41b2-ea37-505ab3a5f708"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@10 ALS = 0.1855\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[visitor_id: int, pred_items: array<int>, pred_k: array<int>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}